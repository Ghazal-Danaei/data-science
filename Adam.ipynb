{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0795a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Regressor:\n",
    "\n",
    "    def __init__(self) -> None: \n",
    "        self.X, self.y = self.generate_dataset(n_samples=200, n_features=1)\n",
    "        n, d = self.X.shape\n",
    "        self.w = np.zeros((d, 1))\n",
    "        self.history = list()\n",
    "        self.weights=np.zeros((d, 1))\n",
    "\n",
    "    def generate_dataset(self, n_samples, n_features):\n",
    "        \"\"\"\n",
    "        Generates a regression dataset\n",
    "        Returns:\n",
    "            X: a numpy.ndarray of shape (100, 2) containing the dataset\n",
    "            y: a numpy.ndarray of shape (100, 1) containing the labels\n",
    "        \"\"\"\n",
    "        from sklearn.datasets import make_regression\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=30)\n",
    "        y = y.reshape(n_samples, 1)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def linear_regression(self):\n",
    "        \"\"\"\n",
    "        Performs linear regression on a dataset\n",
    "        Returns:\n",
    "            y: a numpy.ndarray of shape (n, 1) containing the predictions\n",
    "        \"\"\"\n",
    "        y = np.dot(self.X, self.w)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the labels for a given dataset\n",
    "        X: a numpy.ndarray of shape (n, d) containing the dataset\n",
    "        Returns:\n",
    "            y: a numpy.ndarray of shape (n,) containing the predictions\n",
    "        \"\"\"\n",
    "        y = np.dot(X, self.w).reshape(X.shape[0])\n",
    "        return y\n",
    "\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Computes the MSE loss of a prediction\n",
    "        Returns:\n",
    "            loss: the loss of the prediction\n",
    "        \"\"\"\n",
    "        predictions = self.linear_regression()\n",
    "        loss = np.mean((predictions - self.y)**2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def compute_gradient(self):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the MSE loss\n",
    "        Returns:\n",
    "            grad: the gradient of the loss with respect to w\n",
    "        \"\"\"\n",
    "        predictions = self.linear_regression()\n",
    "        dif = (predictions - self.y)\n",
    "        grad = 2 * np.dot(self.X.T, dif)\n",
    "        return grad\n",
    "\n",
    "\n",
    "    def fit(self, optimizer=\"adam\", n_iters=1000, render_animation=False):\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "        optimizer: the optimization algorithm to use\n",
    "        X: a numpy.ndarray of shape (n, d) containing the dataset\n",
    "        y: a numpy.ndarray of shape (n, 1) containing the labels\n",
    "        n_iters: the number of iterations to train for\n",
    "        \"\"\"        \n",
    "\n",
    "        figs = []\n",
    "\n",
    "        for i in range(1, n_iters+1):\n",
    "            \n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                # TODO: implement gradient descent\n",
    "                self.weights = self.gradient_descent(alpha=0.01)\n",
    "                \n",
    "                \n",
    "            elif optimizer == \"sgd\":\n",
    "                # TODO: implement stochastic gradient descent\n",
    "                self.sgd_optimizer(alpha=0.01)\n",
    "                \n",
    "            elif optimizer == \"sgdMomentum\":\n",
    "                # TODO: Implement the SGD with momentum\n",
    "                self.sgd_momentum(alpha=0.01, momentum=0.7)\n",
    "                \n",
    "            elif optimizer == \"adagrad\":\n",
    "                # TODO: Implement Adagrad\n",
    "                self.adagrad_optimizer(g=0, alpha=0.3, epsilon=10**-8 )\n",
    "                \n",
    "            elif optimizer == \"rmsprop\":\n",
    "                # TODO: implement RMSprop\n",
    "                self.rmsprop_optimizer(g=0, alpha=0.3, beta=0.7, epsilon=10**-8)\n",
    "                \n",
    "            elif optimizer == \"adam\":\n",
    "                # TODO: implement Adam optimizer\n",
    "                self.adam_optimizer(m=0, v=0, alpha=0.3, beta1=0.9, beta2=0.999, epsilon=10**-8, iter_num=i)\n",
    "\n",
    "            # TODO: implement the stop criterion\n",
    "            if math.isinf(float(self.compute_loss())):\n",
    "                    break\n",
    "                    \n",
    "                 \n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\"Iteration: \", i)\n",
    "                J = self.compute_loss()\n",
    "                self.history.append(J)\n",
    "                print(\"Loss: \", J)\n",
    "            \n",
    "            if render_animation:\n",
    "                import matplotlib.pyplot as plt\n",
    "                from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "\n",
    "                fig = plt.figure()\n",
    "                plt.scatter(self.X, self.y, color='red')\n",
    "                plt.plot(self.X, self.predict(self.X), color='blue')\n",
    "                plt.xlim(self.X.min(), self.X.max())\n",
    "                plt.ylim(self.y.min(), self.y.max())\n",
    "                plt.title(f'Optimizer:{optimizer}\\nIteration: {i}')\n",
    "                plt.close()\n",
    "                figs.append(mplfig_to_npimage(fig))\n",
    "            \n",
    "        \n",
    "        if render_animation and len(figs) > 0:\n",
    "            from moviepy.editor import ImageSequenceClip\n",
    "            clip = ImageSequenceClip(figs, fps=5)\n",
    "            clip.write_gif(f'{optimizer}_animation.gif', fps=5)\n",
    "\n",
    "\n",
    "    def gradient_descent(self, alpha):\n",
    "        \"\"\"\n",
    "        Performs gradient descent to optimize the weights\n",
    "        alpha: the learning rate\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "        \"\"\"\n",
    "        w=self.w\n",
    "        # TODO: implement gradient descent\n",
    "        n, d = self.X.shape\n",
    "        diff=np.dot(self.X,self.w)-self.y\n",
    "        TEMP=np.dot(self.X.T,diff)\n",
    "        coef=alpha/n\n",
    "        self.w=self.w-coef*TEMP\n",
    "\n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def sgd_optimizer(self, alpha):\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent to optimize the weights\n",
    "        alpha: the learning rate\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "        \"\"\"\n",
    "        w=self.w\n",
    "        gradian = self.compute_gradient()\n",
    "        # TODO: implement gradient descent\n",
    "        n, d = self.X.shape\n",
    "#         diff=np.dot(self.X,self.w)-self.y\n",
    "#         TEMP=np.dot(self.X.T,diff)\n",
    "        coef=alpha/n\n",
    "        self.w=self.w-coef*gradian\n",
    "\n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def sgd_momentum(self, alpha=0.01, momentum=0.7):\n",
    "        \"\"\"\n",
    "        Performs SGD with momentum to optimize the weights\n",
    "        alpha: the learning rate\n",
    "        momentum: the momentum\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "        \"\"\"\n",
    "        change = 0.0\n",
    "        w = self.w\n",
    "        # TODO: implement stochastic gradient descent\n",
    "        gradient=self.compute_gradient()\n",
    "        change = 0.0\n",
    "        new_change = alpha * gradient + momentum * change\n",
    "        self.w=self.w-new_change\n",
    "        change =new_change\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    \n",
    "    def adagrad_optimizer(self, g, alpha, epsilon):\n",
    "        \n",
    "        \"\"\"\n",
    "        Performs Adagrad optimization to optimize the weights\n",
    "        alpha: the learning rate\n",
    "        epsilon: a small number to avoid division by zero\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "            ...\n",
    "        \"\"\"\n",
    "        w = self.w\n",
    "        # TODO: implement stochastic gradient descent\n",
    "        gradient=self.compute_gradient()\n",
    "#         squered_grad = np.zeros(gradient.shape)\n",
    "#         new_squered_grad =  np.power(gradient,2) \n",
    "        new_g= g + np.power(gradient,2) \n",
    "        temp = np.sqrt(new_g  )        \n",
    "        self.w=self.w -alpha*(gradient/temp+ epsilon)\n",
    "        g = new_g\n",
    "        return self.w\n",
    "\n",
    "    \n",
    "    def rmsprop_optimizer(self, g, alpha, beta, epsilon):\n",
    "        \"\"\"\n",
    "        Performs RMSProp optimization to optimize the weights\n",
    "        g: sum of squared gradients\n",
    "        alpha: the learning rate\n",
    "        beta: the momentum\n",
    "        epsilon: a small number to avoid division by zero\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "            ...\n",
    "        \"\"\"\n",
    "        w = self.w\n",
    "        # TODO: implement stochastic gradient descent\n",
    "        gradient=self.compute_gradient()\n",
    "        \n",
    "        new_g= beta*g + (1-beta)*np.power(gradient,2) \n",
    "        temp = np.sqrt(new_g  )        \n",
    "        self.w=self.w -alpha*(gradient/temp+ epsilon)\n",
    "        g = new_g\n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def adam_optimizer(self, m, v, alpha, beta1, beta2, epsilon, iter_num):\n",
    "        \"\"\"\n",
    "        Performs Adam optimization to optimize the weights\n",
    "        m: the first moment vector\n",
    "        v: the second moment vector\n",
    "        alpha: the learning rate\n",
    "        beta1: the first momentum\n",
    "        beta2: the second momentum\n",
    "        epsilon: a small number to avoid division by zero\n",
    "        Returns:\n",
    "            w: a numpy.ndarray of shape (d, 1) containing the optimized weights\n",
    "            ...\n",
    "        \"\"\"\n",
    "        w = self.w\n",
    "        # TODO: implement stochastic gradient descent\n",
    "        gradient=self.compute_gradient()\n",
    "        \n",
    "        new_m= beta1*m + (1-beta1)*gradient\n",
    "        new_v= beta2*v + (1-beta2)*np.power(gradient,2)\n",
    "        \n",
    "        m = new_m\n",
    "        v = new_v\n",
    "        \n",
    "        \n",
    "        m_corrected =m/(1-beta1**iter_num)\n",
    "        v_corrected =v/(1-beta2**iter_num)\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp = np.sqrt(v_corrected)        \n",
    "        self.w=self.w -alpha*(m_corrected/temp+ epsilon)\n",
    "        \n",
    "        return self.w\n",
    "\n",
    "\n",
    "#     def plot_gradient(self):\n",
    "#         \"\"\"\n",
    "#         Plots the gradient descent path for the loss function\n",
    "#         Useful links: \n",
    "#         -   http://www.adeveloperdiary.com/data-science/how-to-visualize-gradient-descent-using-contour-plot-in-python/\n",
    "#         -   https://www.youtube.com/watch?v=zvp8K4iX2Cs&list=LL&index=2\n",
    "#         \"\"\"\n",
    "#         # TODO: Bonus!\n",
    "#         # grid over which we will calculate J\n",
    "# #         theta0_vals = np.linspace(-10, 10, 100)\n",
    "# #         theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "#         # initialize J_vals to a matrix of 0's\n",
    "#         J_vals = self.history\n",
    "\n",
    "# #         # Fill out J_vals\n",
    "# #         for i, theta0 in enumerate(theta0_vals):\n",
    "# #             for j, theta1 in enumerate(theta1_vals):\n",
    "# #                 J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n",
    "\n",
    "       \n",
    "# #         J_vals = J_vals.T\n",
    "\n",
    "#         # surface plot\n",
    "#         fig = pyplot.figure(figsize=(12, 5))\n",
    "#         ax = fig.add_subplot(121, projection='3d')\n",
    "#         ax.plot_surface(self.weights, self.history, cmap='viridis')\n",
    "#         pyplot.xlabel('theta0')\n",
    "#         pyplot.ylabel('theta1')\n",
    "#         pyplot.title('Surface')\n",
    "\n",
    "        \n",
    "#         ax = pyplot.subplot(122)\n",
    "#         pyplot.contour(self.weights, self.history, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))\n",
    "#         pyplot.xlabel('theta0')\n",
    "\n",
    "#         pyplot.plot(self.weights , 'ro', ms=10, lw=2)\n",
    "#         pyplot.title('Contour, showing minimum')\n",
    "#         pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f58f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  10\n",
      "Loss:  7120.4637724083805\n",
      "Iteration:  20\n",
      "Loss:  6907.244158003389\n",
      "Iteration:  30\n",
      "Loss:  6677.092566674425\n",
      "Iteration:  40\n",
      "Loss:  6424.156026588362\n",
      "Iteration:  50\n",
      "Loss:  6149.717994214673\n",
      "Iteration:  60\n",
      "Loss:  5857.0917597460375\n",
      "Iteration:  70\n",
      "Loss:  5550.077707476951\n",
      "Iteration:  80\n",
      "Loss:  5232.425481357666\n",
      "Iteration:  90\n",
      "Loss:  4907.672793404865\n",
      "Iteration:  100\n",
      "Loss:  4579.12340455296\n",
      "Iteration:  110\n",
      "Loss:  4249.869374398241\n",
      "Iteration:  120\n",
      "Loss:  3922.8222367334447\n",
      "Iteration:  130\n",
      "Loss:  3600.741676376906\n",
      "Iteration:  140\n",
      "Loss:  3286.2590914425637\n",
      "Iteration:  150\n",
      "Loss:  2981.89627039719\n",
      "Iteration:  160\n",
      "Loss:  2690.0801001133123\n",
      "Iteration:  170\n",
      "Loss:  2413.1542164314833\n",
      "Iteration:  180\n",
      "Loss:  2153.388333613759\n",
      "Iteration:  190\n",
      "Loss:  1912.9858085578621\n",
      "Iteration:  200\n",
      "Loss:  1694.08985055113\n",
      "Iteration:  210\n",
      "Loss:  1498.788679651622\n",
      "Iteration:  220\n",
      "Loss:  1329.1198589528647\n",
      "Iteration:  230\n",
      "Loss:  1187.0739699868354\n",
      "Iteration:  240\n",
      "Loss:  1074.5977599932166\n",
      "Iteration:  250\n",
      "Loss:  993.5968601536516\n",
      "Iteration:  260\n",
      "Loss:  945.9381519678334\n",
      "Iteration:  270\n",
      "Loss:  933.0489157518433\n",
      "Iteration:  280\n",
      "Loss:  933.0502530759886\n",
      "Iteration:  290\n",
      "Loss:  933.0515792007345\n",
      "Iteration:  300\n",
      "Loss:  933.0528941036968\n",
      "Iteration:  310\n",
      "Loss:  933.0541977744762\n",
      "Iteration:  320\n",
      "Loss:  933.0554902131917\n",
      "Iteration:  330\n",
      "Loss:  933.0567714292171\n",
      "Iteration:  340\n",
      "Loss:  933.0580414400893\n",
      "Iteration:  350\n",
      "Loss:  933.0593002705616\n",
      "Iteration:  360\n",
      "Loss:  933.0605479517753\n",
      "Iteration:  370\n",
      "Loss:  933.0617845205386\n",
      "Iteration:  380\n",
      "Loss:  933.0630100186909\n",
      "Iteration:  390\n",
      "Loss:  933.0642244925428\n",
      "Iteration:  400\n",
      "Loss:  933.0654279923821\n",
      "Iteration:  410\n",
      "Loss:  933.066620572034\n",
      "Iteration:  420\n",
      "Loss:  933.0678022884728\n",
      "Iteration:  430\n",
      "Loss:  933.0689732014725\n",
      "Iteration:  440\n",
      "Loss:  933.0701333732973\n",
      "Iteration:  450\n",
      "Loss:  933.0712828684223\n",
      "Iteration:  460\n",
      "Loss:  933.0724217532849\n",
      "Iteration:  470\n",
      "Loss:  933.0735500960582\n",
      "Iteration:  480\n",
      "Loss:  933.0746679664502\n",
      "Iteration:  490\n",
      "Loss:  933.0757754355201\n",
      "Iteration:  500\n",
      "Loss:  933.0768725755124\n",
      "Iteration:  510\n",
      "Loss:  933.077959459708\n",
      "Iteration:  520\n",
      "Loss:  933.0790361622873\n",
      "Iteration:  530\n",
      "Loss:  933.080102758207\n",
      "Iteration:  540\n",
      "Loss:  933.0811593230878\n",
      "Iteration:  550\n",
      "Loss:  933.0822059331119\n",
      "Iteration:  560\n",
      "Loss:  933.0832426649297\n",
      "Iteration:  570\n",
      "Loss:  933.0842695955741\n",
      "Iteration:  580\n",
      "Loss:  933.0852868023832\n",
      "Iteration:  590\n",
      "Loss:  933.086294362929\n",
      "Iteration:  600\n",
      "Loss:  933.0872923549515\n",
      "Iteration:  610\n",
      "Loss:  933.0882808563\n",
      "Iteration:  620\n",
      "Loss:  933.0892599448777\n",
      "Iteration:  630\n",
      "Loss:  933.0902296985915\n",
      "Iteration:  640\n",
      "Loss:  933.0911901953066\n",
      "Iteration:  650\n",
      "Loss:  933.0921415128033\n",
      "Iteration:  660\n",
      "Loss:  933.0930837287387\n",
      "Iteration:  670\n",
      "Loss:  933.0940169206114\n",
      "Iteration:  680\n",
      "Loss:  933.0949411657278\n",
      "Iteration:  690\n",
      "Loss:  933.0958565411731\n",
      "Iteration:  700\n",
      "Loss:  933.0967631237831\n",
      "Iteration:  710\n",
      "Loss:  933.097660990119\n",
      "Iteration:  720\n",
      "Loss:  933.0985502164439\n",
      "Iteration:  730\n",
      "Loss:  933.0994308787014\n",
      "Iteration:  740\n",
      "Loss:  933.1003030524969\n",
      "Iteration:  750\n",
      "Loss:  933.1011668130783\n",
      "Iteration:  760\n",
      "Loss:  933.1020222353204\n",
      "Iteration:  770\n",
      "Loss:  933.1028693937094\n",
      "Iteration:  780\n",
      "Loss:  933.1037083623296\n",
      "Iteration:  790\n",
      "Loss:  933.1045392148502\n",
      "Iteration:  800\n",
      "Loss:  933.1053620245146\n",
      "Iteration:  810\n",
      "Loss:  933.1061768641289\n",
      "Iteration:  820\n",
      "Loss:  933.1069838060538\n",
      "Iteration:  830\n",
      "Loss:  933.1077829221941\n",
      "Iteration:  840\n",
      "Loss:  933.1085742839932\n",
      "Iteration:  850\n",
      "Loss:  933.1093579624238\n",
      "Iteration:  860\n",
      "Loss:  933.1101340279837\n",
      "Iteration:  870\n",
      "Loss:  933.1109025506885\n",
      "Iteration:  880\n",
      "Loss:  933.1116636000675\n",
      "Iteration:  890\n",
      "Loss:  933.1124172451589\n",
      "Iteration:  900\n",
      "Loss:  933.1131635545058\n",
      "Iteration:  910\n",
      "Loss:  933.1139025961539\n",
      "Iteration:  920\n",
      "Loss:  933.1146344376465\n",
      "Iteration:  930\n",
      "Loss:  933.1153591460237\n",
      "Iteration:  940\n",
      "Loss:  933.1160767878199\n",
      "Iteration:  950\n",
      "Loss:  933.1167874290616\n",
      "Iteration:  960\n",
      "Loss:  933.1174911352666\n",
      "Iteration:  970\n",
      "Loss:  933.1181879714427\n",
      "Iteration:  980\n",
      "Loss:  933.1188780020871\n",
      "Iteration:  990\n",
      "Loss:  933.1195612911855\n",
      "Iteration:  1000\n",
      "Loss:  933.1202379022134\n"
     ]
    }
   ],
   "source": [
    "R=Regressor()\n",
    "R.fit()\n",
    "# R.plot_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f08a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
